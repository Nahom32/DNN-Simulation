{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMztSrKxhfYGMogrILSS1fw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nahom32/DNN-Simulation/blob/main/DNN_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPnddygXCSqD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Neural Networks\n",
        "This notebook contains an illustration of how the deep neural networks work. It implements how both forward and backward propagation works using the gradient descent algorithm.\n",
        "\n",
        "The forward propagation algorithm works as follows:\n",
        "$$Z^l = W^la^{l-1} + b^l$$\n",
        "$$a^l = \\alpha(Z^l)$$ where $\\alpha$ is the activation function\n",
        "The backward propagation algorithms uses gradient descent to autocorrect the weights. The algorihtm works as follows:\n",
        "$$W^l = W^l - \\frac{∂E}{∂W^l}$$\n",
        "\n",
        "Since the Error equation isn't described interms of the internal weights, we need to use the chain rule to find the partial derivative.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1Dw7oqmhCiRq"
      }
    }
  ]
}