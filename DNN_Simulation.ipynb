{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLNNv0uC67/xBSgofeoo9Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nahom32/DNN-Simulation/blob/main/DNN_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Neural Networks\n",
        "This notebook contains an illustration of how the deep neural networks work. It implements how both forward and backward propagation works using the gradient descent algorithm.\n",
        "\n",
        "The forward propagation algorithm works as follows:\n",
        "$$Z^l = W^la^{l-1} + b^l$$\n",
        "$$a^l = \\alpha(Z^l)$$ where $\\alpha$ is the activation function\n",
        "The backward propagation algorithms uses gradient descent to autocorrect the weights. The algorihtm works as follows:\n",
        "$$W^l = W^l - \\frac{∂E}{∂W^l}$$\n",
        "\n",
        "Since the Error equation isn't described interms of the internal weights, we need to use the chain rule to find the partial derivative.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1Dw7oqmhCiRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n"
      ],
      "metadata": {
        "id": "xxRn_-3v1fF_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SPnddygXCSqD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "  '''\n",
        "  Description\n",
        "  ---------------------------\n",
        "  This is the relu activation function which is mostly used in hidden layers\n",
        "  ===========================\n",
        "  args:\n",
        "    x: flloat, np.matrix\n",
        "  returns:\n",
        "    np.matrix\n",
        "  '''\n",
        "  return np.maximum(x,0)\n",
        "\n",
        "def softmax(x):\n",
        "    x_val = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return x_val / np.sum(x_val, axis=1, keepdims=True)\n",
        "\n",
        "def sigmoid(x):\n",
        "  '''\n",
        "  Description\n",
        "  ---------------------------\n",
        "  This is the sigmoid activation function\n",
        "  ===========================\n",
        "  args:\n",
        "    x: flloat, np.matrix\n",
        "  returns:\n",
        "    np.matrix\n",
        "  '''\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qHawWAzJ2g9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BoDvGJGu4Sf7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}